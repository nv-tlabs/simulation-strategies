<!doctype html>
<html lang="en">
  <head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-R4V1SVXJM8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-R4V1SVXJM8');
  </script>


    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Other -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.4.2/handlebars.min.js"></script>

    <title>Towards Optimal Strategies for Training Self-Driving Perception Models in Simulation </title>
  </head>
  <body>

    <div style="overflow: hidden; background-color: #7689de;">
      <div class="container">
      <a href=https://www.nvidia.com/ style="float: left; color: black; text-align: center; padding: 12px 16px; text-decoration: none; font-size: 16px;"><img width="100%" src="https://nv-tlabs.github.io/3DStyleNet/assets/nvidia.svg"></a>
      <a href=https://nv-tlabs.github.io/ style="float: left; color: black; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 16px;"><strong>Toronto AI Lab</strong></a>
      </div>
    </div>

    <!-- header -->
    <div class='jumbotron' style="background-color:#a9dce3">
    <div class="container">

    <h1 class="text-center">Towards Optimal Strategies for Training Self-Driving Perception Models in Simulation</h1>
    <p class='text-center'><a href="http://www.cs.toronto.edu/~davidj/" target="_blank">David Acuna&#42;</a>, <a href="https://scholar.google.com/citations?user=VVIAoY0AAAAJ&hl=en" target="_blank">Jonah Philion&#42;</a>, <a href="http://www.cs.toronto.edu/~fidler/" target="_blank">Sanja Fidler</a></p>
    <p class='text-center'>NVIDIA, University of Toronto, Vector Institute</p>
    <p class='text-center'><b>NeurIPS 2021</b></p>

    <figure class='text-center'><img src='imgs/sim.gif' class='img-fluid' style='height:250px; border-radius:15px; padding:5px'>
    <figcaption>Source Domain (Carla Simulator)</figcaption>
    </figure>
    <figure class='text-center'><img src='imgs/real.gif' class='img-fluid' style='height:250px; border-radius:15px; padding:5px'>
    <figcaption>Target Domain (nuScenes dataset)</figcaption></figure>

    </div>
    </div>

    <div class="container">

      <p class='text-justify'>
        Autonomous driving relies on a huge volume of real-world data to be labeled to high precision.  Alternative solutions seek to exploit driving simulators that can generate large amounts of labeled data with a plethora of content variations. However, the <i>domain gap</i> between the synthetic and real data remains,  raising the following important question:
        <i>What are the best ways to utilize a self-driving simulator for perception tasks?</i> In this work, we build on top of recent advances in domain-adaptation theory, and from this perspective,  propose ways to minimize the reality gap. We primarily focus on the use of labels in the synthetic domain alone.  Our approach introduces both a principled way to learn neural-invariant representations and a theoretically inspired view on how to sample the data from the simulator. Our method is easy to implement in practice as it is agnostic of the network architecture and the choice of the simulator.
        We showcase our approach on the bird's-eye-view vehicle segmentation task with multi-sensor data (cameras, lidar) using an open-source simulator (CARLA), and evaluate the entire framework on a real-world dataset (nuScenes).
        Last but not least, we show what types of variations (e.g. weather conditions, number of assets, map design, and color diversity) matter to perception networks when trained with driving simulators, and which ones can be compensated for with our domain adaptation technique.
      </p>

    <hr/>

    <span class="border border-white">
    <h4 class="text-center">News</h4>
    <ul>
      <li>[Nov 2021] paper released on <a href='https://arxiv.org/abs/2111.07971' target="_blank">arxiv</a></li>
    </ul>
    </span>

    <hr/>
            <center><h4>Paper</h4></center>
            <table align=center width=700>
            <tbody>

                <tr>
                  <td><a href="./"><img style="height:180px; border: solid; border-radius:30px;" src="imgs/icon.png"/></a></td>
                  <td><br style="font-size:16px"> David Acuna<sup>*</sup>, Jonah Philion<sup>*</sup>, Sanja Fidler
                      <br><sup>(* Equal contribution)</sup><br><br>
                          Towards Optimal Strategies for Training <br>Self-Driving Perception Models in Simulation<br><br>
                  NeurIPS, 2021.<br>
                    </td>
              </tr>
              </tbody>
            </table>
            <br>

            <table align=center width=700px>
              <tr>
                  <td>
                    <span style="font-size:18px"><center>
                      <a href="https://arxiv.org/abs/2111.07971">[Arxiv Preprint]</a>
                    </center></span></td>
                    <td>
                      <span style="font-size:18px"><center>
                        <a href="https://openreview.net/forum?id=ZfIO21FYv4">[OpenReview NeurIPS discussions]</a>
                      </center></span></td>

                  <td><span style="font-size:18px"><center>
                      <a href="./cite.txt">[Bibtex]</a>
                    </center></span></td>

                  <td><span style="font-size:18px"><center>
                      <a href="#">[Video Presentation]</a>
                    </center></span></td>


              </tr>
              <tr>

              </tr>
            </table>

    <hr/>
    <center><h4>Our approach in a nutshell</h4></center>

    <div class='row'>
      <div class='col-8 text-justify'>
        <br><h5>1. Synthetic Data Generation</h5></br>
        <p class="text-justify">
        Our synthetic data generation strategy aims to minimize the distance between the labels marginal (P(y)) while at the same time encouraging for diversity in the generated data.
        Importantly, if labels in the target domain are not available, we cannot measure the divergence between the labels marginals between the generated and the real world data.
        That said, given the bird‘s-eye view segmentation map, it is not hard for a human to design spatial priors representing locations with high probability of finding a vehicle.
        As visualized in the figure, we design a simple prior that samples locations for non-player characters (NPCs) proportional to the longitudinal distance of the vehicle to the ego car.</p>
      </div>
      <div class='col-4 text-center'>
      <figure>
      <img src="imgs/toysample.png" width="80%"></img>
      <figcaption>
      <p class="text-justify"> <b>Synthetic Data Generation.</b> <b>(Left)</b> Standard approaches sample NPC locations from the map's drivable area based on the structure of the road. <b>(Right)</b> We sample the NPC locations based on our spatial prior (independent of the structure of the road) which aims for diversity in the generated data and minimizes the divergence between the task label marginals.</figcaption>
      </p>
      </figcaption>
      </figure>
      </div>
    </div>
    <div class="text-center">
        <figure>
        <img src="imgs/ymargin.png" width="90%"></img>
        <figcaption>
        <p class="text-justify">
            <b>Marginals induced by different sampling strategies for the task of BEV vehicle segmentation</b>. Figures <b>(a)</b> and <b>(b)</b> show the induced marginal Ps(y) when the sampling strategy follows standard approaches: e.g. the NPC locations are sampled based on  the structure of the road or the map’s drivable area.
            Figure <b>(c)</b> shows the induced marginal when the NPC locations are sampled using our spatial prior.
            Figure <b>(d)</b> is for reference as it assumes access to target labeled data. It shows the induced marginal when the NPC locations are sampled using a target prior estimated on the nuScenes training set.
            Figure <b>(e)</b> shows the estimated label marginal of the nuScenes validation set for the task of vehicle BEV segmentation Pt(y).
            The number on the top-left of the figures corresponds to the distance between the induced marginal, and the nuScenes validation set estimated marginal (lower is better).
            In comparison with standard approaches, our sampling strategy minimizes the divergence between the task label marginals and leads to a more diverse synthetic dataset.
            Notably, comparing  <b>(c)</b> and  <b>(d)</b>, we can observe their distances are in the same order, yet sampling based on the spatial prior does not require access to labeled data.
        </p>

        </figcaption>
        </figure>
    </div>
    <div class='row'>
    <div class='col-8 text-justify'>
    <br><h5>2. Training Strategy</h5></br>
         <p class="text-justify">
             Our training strategy extends f-DAL (the training algorithm from [3]) to dense prediction tasks, with the aim to learn domain-invariant representations and minimize the divergence between virtual and real world in a latent space Z.
             Minimizing the divergence in a representation space allows domain adaptation algorithms to be sensor and architecture agnostic.
             We additionally take inspiration from [46] and incorporate the use of pseudolabels into the f-DAL framework.
             We emphasize that our data-generation strategy is what enables us to effectively learn invariant representations using the adversarial framework (why? check out the paper!)
         </p>
    </div>
    <div class="col-4 text-justify">
    <figure>
     <img src="imgs/architecture.png" width="100%"></img>
        <figcaption> <b>Lift Splat Adapt</b>.
        We follow the same architecture from Lift-Splat-Shoot [36].
        The discrepancy term dst from f-DAL [3] is computed using a per-location domain classifier hˆ.</figcaption>
    </figure>
    </div>
    </div>
    <hr/>

 <center><h4>Qualitative Results</h4></center>
    <h5 class='text-center'>Lift Splat vs. Lift Splat Adapt (Ours) vs. Oracle</h5>
    <figure>
    <div class='row justify-content-md-center'>
      <iframe src="https://drive.google.com/file/d/1NBFvB7KY76ST_G4_najGANSqTV7C4I0g/preview" width="640" height="480" allow="autoplay"></iframe>
    </div>
    <figcaption>
    <p align="text-center">We visualize the output of Lift Splat (LS) for a scene from nuScenes val.
    The model takes as input the 6 images shown on the left, outputs the heatmap shown on the right.
    We project the prediction back onto the input images colorized according to depth.
    <b>a) Top</b> is a LS model without adaptation, <b>b) Middle</b> is a LS model trained with our method, and <b>c) Bottom</b> is a LS model trained directly on nuScenes.
    </figcaption>
    </p>
    </figure>

  <hr/>

  <center><h4>Analysis</h4></center> <p align='text-justify'>What matters to the self-driving simulator with and without our domain adaptation techinque?</p>
  <p align='text-justify'><b>Camera Realism.</b> We measure the extent to which it is important for images to be photorealistic by toggling CARLA's camera postprocessing (see "RGB Camera" <a href=https://carla.readthedocs.io/en/0.9.11/ref_sensors/)>here</a>). Without any adaptation, the Lift Splat model is unable to generalize to the real world. Adaptation compensates for the loss in photorealism to a large degree.</p>
  <p align='text-justify'><b>Vehicles Per Episode</b> / <b>Car Color Variation.</b> An intuitive goal for self-driving simulators has been to improve the quantity and quality of vehicle assets. We show that performance is sensitive to the number of vehicle assets used in the simulator and the number of NPCs sampled per episode (a dataset with 1 vehicle sampled per episode is visualized in the video), but is not so sensitive to the variance in the colors chosen for the NPCs (a dataset where vehicle colors are sampled uniformly is visualized below).</p>
  <p align='text-justify'><b>Vehicle Assets.</b> We vary the number of vehicle assets used when sampling CARLA data. Lift-Splat-Adapt (ours) is able to compensate for performance when very few assets are used. The video shown below shows a dataset where only 1 vehicle asset is used.</p>
  <p align='text-justify'><b>Weather.</b> nuScenes contains sunny, rainy, and nighttime scenes.  We test the extent to which it’s important for CARLA data to also have variance in weather. CARLA has 15 different "standard" weather settings as well as controls for cloudiness, precipitation, precipitation deposits (e.g. puddles), wind intensity, wetness, fog density, and sun altitude angle (which controls nighttime vs. daytime on a sliding scale). We randomly sample these controls and compare performance against sampling categorically from the 15 preset weather settings. We also compare against exclusively using the "sunny" weather setting (shown in the video). We find that our method is able to compensate for much of the loss in weather diversity and using only the sunny weather setting on data where NPCs are sampled according to the prior achieves higher performance than any amount ofweather diversity with NPCs sampled according to the town.</p>
  <p align='text-justify'><b>Camera Extrinsics and Intrinsics.</b> Instead of sampling camera extrinsics and intrinsics that exactly correspond to extrinsics and intrinsics from nuScenes, we sample yaw (left) and field of view (middle) from nuScenes with some variance. Transfer performance is largely unaffected by adjusting the distribution of intrinsics and extrinsics. Note that generalization across extrinsics and intrinsics is somewhat expected given the design of the Lift Splat Shoot architecture.</p>

    <h4 class="text-center">Camera Realism</h4>
    <div class='row'>
      <div class='col-3 text-center'>
        <img width="100%" src="imgs/camproc.png">
      </div>
      <div class='col-9 text-center'>
      <iframe src="https://drive.google.com/file/d/1-xlr9u-ZEuqKLL8vi1dThfKuhg22ODn5/preview" width="90%" height="250px" allow="autoplay"></iframe>
      </div>
    </div>
    <br>

    <h4 class="text-center">Vehicles Per Episode</h4>
    <div class='row'>
      <div class='col-3 text-center'>
        <img width="100%" src="imgs/vehperep.png">
      </div>
      <div class='col-9 text-center'>
      <iframe src="https://drive.google.com/file/d/1_40M_B1kRtXfAiHxxT_Z0IQKTuE-2CXU/preview" width="90%" height="250px" allow="autoplay"></iframe>
      </div>
    </div>
    <br>

    <h4 class="text-center">Car Color Variation</h4>
    <div class='row'>
      <div class='col-3 text-center'>
        <img width="100%" src="imgs/carcolor.png">
      </div>
      <div class='col-9 text-center'>
        <iframe src="https://drive.google.com/file/d/1Fa8QWFKoPnDR7GbInpRMRjAeB1yo2BC0/preview" width="90%" height="250px" allow="autoplay"></iframe>
      </div>
    </div>
    <br>

    <h4 class="text-center">Vehicle Assets</h4>
    <div class='row'>
      <div class='col-3 text-center'>
        <img width="100%" src="imgs/assets.png">
      </div>
      <div class='col-9 text-center'>
      <iframe src="https://drive.google.com/file/d/1xvT7LUjZ6lRA9yf4kWx9n2H1ixXmEdN3/preview" width="90%" height="250px" allow="autoplay"></iframe>
      </div>
    </div>
    <br>

    <h4 class="text-center">Weather</h4>
    <div class='row'>
      <div class='col-3 text-center'>
        <img width="100%" src="imgs/weather.png">
      </div>
      <div class='col-9 text-center'>
      <iframe src="https://drive.google.com/file/d/1nBaDCLK77P-I79CmkMx5sq_Aks_Gm4Rr/preview" width="90%" height="250px" allow="autoplay"></iframe>
      </div>
    </div>
    <br>

    <h4 class="text-center">Camera Extrinsics and Intrinsics</h4>
    <div class='row justify-content-md-center'>
      <!-- <div class='col-3 text-center align-middle'>
        <img width="100%" src="imgs/carrig.png">
      </div> -->
      <div class='col-3 text-center'>
        <img width='100%' src='imgs/camfov.png'>
      </div>
      <div class='col-3 text-center'>
        <img width='100%' src='imgs/camorient.png'>
      </div>
      <div class='col-3 text-center'>
        <img width='100%' src='imgs/motionblur.png'>
      </div>
    </div>
    <br>

    </div>
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </body>
</html>